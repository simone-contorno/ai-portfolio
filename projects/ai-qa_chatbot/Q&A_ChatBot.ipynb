{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8289c39",
   "metadata": {},
   "source": [
    "# Build an AI RAG Assistant Using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625adc78",
   "metadata": {},
   "source": [
    "Imagine you work as a consultant for Quest Analytics, a small but fast-growing research organization.\n",
    "\n",
    "In todayâ€™s fast-paced research environment, the sheer volume of scientific papers can be overwhelming, making it nearly impossible to stay up-to-date with the latest developments. \n",
    "\n",
    "The researchers at Quest Analytics have been struggling to find the time to examine countless documents, let alone extract the most relevant and insightful information. \n",
    "\n",
    "You have been hired to build an AI RAG assistant that can read, understand, and summarize vast amounts of data, all in real time. Follow the below tasks to construct the AI-powered RAG assistant to optimize the research endeavors at Quest Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c00c8",
   "metadata": {},
   "source": [
    "## Task 1: Load document using LangChain for different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24680bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\Desktop\\Courses\\Coursera\\IBM AI Engineer Professional Certificate\\Course 13 - Project Generative AI Applications with RAG and LangChain\\Labs\\.venv\\Lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
      "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
      "inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some-\n",
      "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
      "BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications. All relevant code an\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_url = \"A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "pages = loader.load_and_split()\n",
    "print(pages[0].page_content[:1000])  # Print the first 1000 characters of the first page for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecca4ed",
   "metadata": {},
   "source": [
    "## Task 2: Apply text splitting techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f564f912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\\\documentclass{article}\\n\\n    \\x08egin{document}\\n\\n    \\\\maketitle\\n\\n    \\\\section{Introduction}'),\n",
       " Document(metadata={}, page_content='Large language models (LLMs) are a type of machine learning model that can be trained on vast'),\n",
       " Document(metadata={}, page_content='amounts of text data to generate human-like language. \\n    In recent years, LLMs have made'),\n",
       " Document(metadata={}, page_content='significant advances in various natural language processing tasks, including language translation,'),\n",
       " Document(metadata={}, page_content='text generation, \\n    and sentiment analysis.\\n\\n    \\\\subsection{History of LLMs}\\n\\n    The earliest'),\n",
       " Document(metadata={}, page_content='LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could'),\n",
       " Document(metadata={}, page_content='be processed and the computational \\n    power available at the time. In the past decade, however,'),\n",
       " Document(metadata={}, page_content='advances in hardware and software have made it possible to train LLMs on massive datasets,'),\n",
       " Document(metadata={}, page_content='leading to significant improvements in performance.\\n\\n    \\\\subsection{Applications of LLMs}'),\n",
       " Document(metadata={}, page_content='LLMs have many applications in the industry, including chatbots, content creation, and virtual'),\n",
       " Document(metadata={}, page_content='assistants. They can also be used in academia for \\n    research in linguistics, psychology, and'),\n",
       " Document(metadata={}, page_content='computational linguistics.\\n\\n    \\\\end{document}')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "latex_text = \"\"\"\n",
    "    \\documentclass{article}\n",
    "\n",
    "    \\begin{document}\n",
    "\n",
    "    \\maketitle\n",
    "\n",
    "    \\section{Introduction}\n",
    "\n",
    "    Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. \n",
    "    In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, \n",
    "    and sentiment analysis.\n",
    "\n",
    "    \\subsection{History of LLMs}\n",
    "\n",
    "    The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational \n",
    "    power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets,\n",
    "    leading to significant improvements in performance.\n",
    "\n",
    "    \\subsection{Applications of LLMs}\n",
    "\n",
    "    LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for \n",
    "    research in linguistics, psychology, and computational linguistics.\n",
    "\n",
    "    \\end{document}\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.LATEX,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([latex_text])\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535fc76",
   "metadata": {},
   "source": [
    "## Task 3: Embed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d493bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "c:\\Users\\simon\\Desktop\\Courses\\Coursera\\IBM AI Engineer Professional Certificate\\Course 13 - Project Generative AI Applications with RAG and LangChain\\Labs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.007003862410783768,\n",
       " 0.010914131067693233,\n",
       " 0.08746250718832016,\n",
       " 0.08679929375648499,\n",
       " 0.026648471131920815]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pydantic import BaseModel\n",
    "\n",
    "## Embedding model\n",
    "def embedding():\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True},\n",
    "    )\n",
    "\n",
    "    return embedding\n",
    "\n",
    "query = \"How are you?\"\n",
    "embedding_model = embedding()\n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "query_embedding[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2233d",
   "metadata": {},
   "source": [
    "## Task 4: Create and configure vector databases to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70df5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content='to our success. We regularly review and update this policy to incorporate best practices in'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='4. Mobile Phone Policy'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='This policy encourages the responsible use of mobile devices in line with legal and ethical'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Consequences: Non-compliance with this policy may result in disciplinary actions, including'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Consequences: Violations of this policy may lead to disciplinary action, including potential')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load text data\n",
    "loader = TextLoader(\"new-Policies.txt\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "# Create embeddings for the chunks\n",
    "ids = [str(i) for i in range(0, len(chunks))]\n",
    "vectordb = Chroma.from_documents(chunks, embedding_model, ids=ids)\n",
    "\n",
    "# Similarity search\n",
    "query = \"Smoking policy\"\n",
    "docs = vectordb.similarity_search(query, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a5eb0",
   "metadata": {},
   "source": [
    "## Task 5: Develop a retriever to fetch document segments based on queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a60938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content='3. Internet and Email Policy'),\n",
       " Document(metadata={'source': 'new-Policies.txt'}, page_content='Compliance: Adhere to all relevant laws and regulations concerning internet and email use,')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Email policy\"\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546dc76",
   "metadata": {},
   "source": [
    "## Task 6: Construct a QA Bot that leverages the LangChain and LLM to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa8174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## LLM\n",
    "def get_llm():\n",
    "    model_id = 'google/flan-t5-large'\n",
    "    \n",
    "    # Get HuggingFace API key from environment\n",
    "    hf_api_key = os.getenv('HUGGING_FACE_API_KEY')\n",
    "    \n",
    "    # Create tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_api_key)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id, token=hf_api_key)\n",
    "    \n",
    "    # Create text generation pipeline\n",
    "    text_generation_pipeline = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.5,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Create LangChain HuggingFace LLM\n",
    "    hf_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    return hf_llm\n",
    "\n",
    "## Document loader\n",
    "def document_loader(file):\n",
    "    loader = PyPDFLoader(file.name)\n",
    "    loaded_document = loader.load()\n",
    "    return loaded_document\n",
    "\n",
    "## Text splitter\n",
    "def text_splitter(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "## Vector db\n",
    "def vector_database(chunks):\n",
    "    embedding_model = hf_embedding()\n",
    "    vectordb = Chroma.from_documents(chunks, embedding_model)\n",
    "    return vectordb\n",
    "\n",
    "## Embedding model\n",
    "def hf_embedding():\n",
    "    \n",
    "    hf_embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True},\n",
    "    )\n",
    "    return hf_embedding\n",
    "\n",
    "## Retriever\n",
    "def retriever(file):\n",
    "    splits = document_loader(file)\n",
    "    chunks = text_splitter(splits)\n",
    "    vectordb = vector_database(chunks)\n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "## QA Chain\n",
    "def retriever_qa(file, query):\n",
    "    llm = get_llm()\n",
    "    retriever_obj = retriever(file)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=retriever_obj, \n",
    "                                    return_source_documents=False)\n",
    "    response = qa.invoke(query)\n",
    "    return response['result']\n",
    "\n",
    "# Create Gradio interface\n",
    "rag_application = gr.Interface(\n",
    "    fn=retriever_qa,\n",
    "    allow_flagging=\"never\",\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),  # Drag and drop file upload\n",
    "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Output\"),\n",
    "    title=\"RAG Chatbot\",\n",
    "    description=\"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "rag_application.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
